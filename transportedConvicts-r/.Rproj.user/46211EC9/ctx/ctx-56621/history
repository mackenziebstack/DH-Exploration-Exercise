dtm <- tc_words %>%
cast_dtm(id, word, n)
ibrary(tidyverse)
library(tidytext)
library("dplyr")
library(tm)
tc  <- read_csv('trasportedConvicts.csv')
install.packages('tidyverse')
install.packages('tidytext')
install.packages('dplyr')
install.packages("slam")
install.packages("tidytext")
install.packages("dplyr")
install.packages("slam")
library(tidyverse)
library(tidytext)
library("dplyr")
library(tm)
tc  <- read_csv('trasportedConvicts.csv')
tc_df <- tibble(id = tc$id, text = (str_remove_all(tc$trialText, "[0-9]")), date = tc$crimeData)
tidy_tc <- tc_df %>%
unnest_tokens(word, text)
data(stop_words)
tidy_tc <- tidy_tc %>%
anti_join(stop_words)
tc_words <- tidy_tc %>%
count(id, word, sort = TRUE)
head(tc_words)
dtm <- tc_words %>%
cast_dtm(id, word, n)
require(topicmodels)
# number of topics
K <- 15
# set random number generator seed
# for purposes of reproducibility
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(dtm, K, method="Gibbs", control=list(iter = 500, verbose = 25))
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(dtm, K, method="Gibbs", control=list(iter = 500, verbose = 25))
install.packages("topicmodels")
library(topicmodels)
require(topicmodels)
# number of topics
K <- 15
# set random number generator seed
# for purposes of reproducibility
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(dtm, K, method="Gibbs", control=list(iter = 500, verbose = 25))
# have a look a some of the results (posterior distributions)
tmResult <- posterior(topicModel)
# format of the resulting object
attributes(tmResult)
# lengthOfVocab
ncol(dtm)
beta <- tmResult$terms   # get beta from results
dim(beta)
theta <- tmResult$topics
dim(theta)
top5termsPerTopic <- terms(topicModel, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")
topicNames
library("reshape2")
install.packages("reshape2")
install.packages("ggplot2")
install.packages("ggplot2")
library("ggplot2")
exampleIds <- c(2, 100, 200)
N <- length(exampleIds)
# get topic proportions form example documents
topicProportionExamples <- theta[exampleIds,]
colnames(topicProportionExamples) <- topicNames
# put the data into a dataframe just for our visualization
vizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = "topic", id.vars = "document")
exampleIds <- c(2, 100, 200)
N <- length(exampleIds)
# get topic proportions form example documents
topicProportionExamples <- theta[exampleIds,]
nrow(x)
install.packages('tidyverse')
install.packages('tidytext')
install.packages('dplyr')
install.packages("slam")
install.packages("topicmodels")
install.packages("reshape2")
install.packages("ggplot2")
# libraries
library(tidyverse)
library(tidytext)
library("dplyr")
library(tm)
library(topicmodels)
library("reshape2")
library("ggplot2")
# load, clean, and get data into shape
tc  <- read_csv('trasportedConvicts.csv')
#tc_df <- tc_df %>% mutate(id = row_number())
tc_df <- tibble(id = tc$id, text = (str_remove_all(tc$trialText, "[0-9]")), date = tc$crimeData)
tidy_tc <- tc_df %>%
unnest_tokens(word, text)
data(stop_words)
tidy_tc <- tidy_tc %>%
anti_join(stop_words)
tc_words <- tidy_tc %>%
count(id, word, sort = TRUE)
head(tc_words)
dtm <- tc_words %>%
cast_dtm(id, word, n)
require(topicmodels)
# number of topics
K <- 15
# set random number generator seed
# for purposes of reproducibility
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(dtm, K, method="Gibbs", control=list(iter = 500, verbose = 25))
# have a look a some of the results (posterior distributions)
tmResult <- posterior(topicModel)
# format of the resulting object
attributes(tmResult)
# lengthOfVocab
ncol(dtm)
beta <- tmResult$terms   # get beta from results
dim(beta)
theta <- tmResult$topics
dim(theta)
top5termsPerTopic <- terms(topicModel, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")
topicNames
exampleIds <- c(2, 100, 200)
N <- length(exampleIds)
# get topic proportions form example documents
topicProportionExamples <- theta[exampleIds,]
colnames(topicProportionExamples) <- topicNames
# put the data into a dataframe just for our visualization
vizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = "topic", id.vars = "document")
install.packages("tidyverse")
install.packages("tidytext")
install.packages("dplyr")
install.packages("slam")
install.packages("topicmodels")
install.packages("slam")
install.packages("topicmodels")
tc  <- read_csv('trasportedConvicts.csv')
tc_df <- tibble(id = tc$id, text = (str_remove_all(tc$trialText, "[0-9]")), date = tc$crimeData)
tidy_tc <- tc_df %>%
unnest_tokens(word, text)
data(stop_words)
tidy_tc <- tidy_tc %>%
anti_join(stop_words)
tc_words <- tidy_tc %>%
count(id, word, sort = TRUE)
head(tc_words)
dtm <- tc_words %>%
cast_dtm(id, word, n)
require(topicmodels)
# number of topics
K <- 15
# set random number generator seed
# for purposes of reproducibility
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(dtm, K, method="Gibbs", control=list(iter = 500, verbose = 25))
# have a look a some of the results (posterior distributions)
tmResult <- posterior(topicModel)
# format of the resulting object
attributes(tmResult)
# lengthOfVocab
ncol(dtm)
beta <- tmResult$terms   # get beta from results
dim(beta)
theta <- tmResult$topics
dim(theta)
top5termsPerTopic <- terms(topicModel, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")
topicNames
exampleIds <- c(2, 100, 200)
N <- length(exampleIds)
# get topic proportions form example documents
topicProportionExamples <- theta[exampleIds,]
exampleIds <- c(2, 10, 20)
# get topic proportions form example documents
topicProportionExamples <- theta[exampleIds,]
colnames(topicProportionExamples) <- topicNames
# put the data into a dataframe just for our visualization
vizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = "topic", id.vars = "document")
ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = "proportion") +
geom_bar(stat="identity") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
coord_flip() +
facet_wrap(~ document, ncol = N)
#topics over time
# append decade information for aggregation
tc$decade <- paste0(substr(tc$date, 0, 3), "0")
tc_df <- tibble(id = tc$id, text = (str_remove_all(tc$trialText, "[0-9]")), date = tc$crimeDate)
tc  <- read_csv('trasportedConvicts.csv')
tc_df <- tibble(id = tc$id, text = (str_remove_all(tc$trialText, "[0-9]")), date = tc$crimeDate)
tidy_tc <- tc_df %>%
unnest_tokens(word, text)
data(stop_words)
tidy_tc <- tidy_tc %>%
anti_join(stop_words)
tc_words <- tidy_tc %>%
count(id, word, sort = TRUE)
head(tc_words)
dtm <- tc_words %>%
cast_dtm(id, word, n)
require(topicmodels)
# number of topics
K <- 15
# set random number generator seed
# for purposes of reproducibility
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(dtm, K, method="Gibbs", control=list(iter = 500, verbose = 25))
# have a look a some of the results (posterior distributions)
tmResult <- posterior(topicModel)
# format of the resulting object
attributes(tmResult)
# lengthOfVocab
ncol(dtm)
beta <- tmResult$terms   # get beta from results
dim(beta)
theta <- tmResult$topics
dim(theta)
top5termsPerTopic <- terms(topicModel, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")
topicNames
exampleIds <- c(2, 10, 20)
N <- length(exampleIds)
# get topic proportions form example documents
topicProportionExamples <- theta[exampleIds,]
colnames(topicProportionExamples) <- topicNames
# put the data into a dataframe just for our visualization
vizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = "topic", id.vars = "document")
ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = "proportion") +
geom_bar(stat="identity") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
coord_flip() +
facet_wrap(~ document, ncol = N)
#topics over time
# append decade information for aggregation
tc$decade <- paste0(substr(tc$date, 0, 3), "0")
#topics over time
# append decade information for aggregation
tc$decade <- paste0(substr(tc$crimeDate, 0, 3), "0")
# get mean topic proportions per decade
topic_proportion_per_decade <- aggregate(theta, by = list(decade = tc$decade), mean)
# set topic names to aggregated columns
colnames(topic_proportion_per_decade)[2:(K+1)] <- topicNames
vizDataFrame <- melt(topic_proportion_per_decade, id.vars = "decade")
require(pals)
library("pals")
require(pals)
ggplot(vizDataFrame, aes(x=decade, y=value, fill=variable)) +
geom_bar(stat = "identity") + ylab("proportion") +
scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "decade") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
install.packages('tidyverse')
install.packages('tidytext')
install.packages('dplyr')
install.packages("slam")
install.packages("topicmodels")
install.packages("reshape2")
install.packages("ggplot2")
install.packages("pals")
library(tidyverse)
library(tidytext)
library("dplyr")
library(tm)
library(topicmodels)
library("reshape2")
library("ggplot2")
library("pals")
tc  <- read_csv('trasportedConvicts.csv')
tc_df <- tibble(id = tc$id, text = (str_remove_all(tc$trialText, "[0-9]")), date = tc$crimeDate)
tidy_tc <- tc_df %>%
unnest_tokens(word, text)
data(stop_words)
tidy_tc <- tidy_tc %>%
anti_join(stop_words)
tc_words <- tidy_tc %>%
count(id, word, sort = TRUE)
head(tc_words)
dtm <- tc_words %>%
cast_dtm(id, word, n)
require(topicmodels)
# number of topics
K <- 15
# set random number generator seed
# for purposes of reproducibility
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(dtm, K, method="Gibbs", control=list(iter = 500, verbose = 25))
# have a look a some of the results (posterior distributions)
tmResult <- posterior(topicModel)
# format of the resulting object
attributes(tmResult)
# lengthOfVocab
ncol(dtm)
beta <- tmResult$terms   # get beta from results
dim(beta)
theta <- tmResult$topics
dim(theta)
top5termsPerTopic <- terms(topicModel, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")
topicNames
exampleIds <- c(2, 10, 20)
N <- length(exampleIds)
# get topic proportions form example documents
topicProportionExamples <- theta[exampleIds,]
colnames(topicProportionExamples) <- topicNames
# put the data into a dataframe just for our visualization
vizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = "topic", id.vars = "document")
ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = "proportion") +
geom_bar(stat="identity") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
coord_flip() +
facet_wrap(~ document, ncol = N)
#topics over time
# append decade information for aggregation
tc$decade <- paste0(substr(tc$crimeDate, 0, 3), "0")
# get mean topic proportions per decade
topic_proportion_per_decade <- aggregate(theta, by = list(decade = tc$decade), mean)
# set topic names to aggregated columns
colnames(topic_proportion_per_decade)[2:(K+1)] <- topicNames
vizDataFrame <- melt(topic_proportion_per_decade, id.vars = "decade")
require(pals)
ggplot(vizDataFrame, aes(x=decade, y=value, fill=variable)) +
geom_bar(stat = "identity") + ylab("proportion") +
scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "decade") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
install.packages('tidyverse')
install.packages('tidytext')
install.packages('dplyr')
install.packages("slam")
install.packages("topicmodels")
install.packages("reshape2")
install.packages("ggplot2")
install.packages("pals")
# libraries
library(tidyverse)
library(tidytext)
library("dplyr")
library(tm)
library(topicmodels)
library("reshape2")
library("ggplot2")
library("pals")
install.packages("tidyverse")
install.packages("tidytext")
install.packages("dplyr")
install.packages("slam")
install.packages("topicmodels")
install.packages("reshape2")
install.packages("ggplot2")
install.packages("pals")
install.packages("topicmodels")
install.packages("reshape2")
install.packages("ggplot2")
install.packages("pals")
install.packages("pals")
tc  <- read_csv('corporalConvicts.csv')
tc_df <- tibble(id = tc$id, text = (str_remove_all(tc$trialText, "[0-9]")), date = tc$crimeDate)
tidy_tc <- tc_df %>%
unnest_tokens(word, text)
data(stop_words)
install.packages('tidyverse')
install.packages('tidytext')
install.packages('dplyr')
install.packages("slam")
install.packages("topicmodels")
install.packages("reshape2")
install.packages("ggplot2")
install.packages("pals")
# libraries
library(tidyverse)
library(tidytext)
library("dplyr")
library(tm)
library(topicmodels)
library("reshape2")
library("ggplot2")
library("pals")
install.packages("tidyverse")
